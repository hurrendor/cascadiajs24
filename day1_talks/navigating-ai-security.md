---
id: omm4s4jx2a1r43jyb77oc37
title: Navigating Ai Security
desc: ''
updated: 1718900841900
created: 1718900810012
---
# How to Navigate Emerging AI-Driven Security
## Logan Gore

### Speaker Information
[LinkedIn](https://www.linkedin.com/in/logan-gore/) 

## Primary security goals
1. Securing your application against abuse
2. Protecting your customers from abuse

76% of attacks have some social/human element.

Pre-AI Hacking needed some expertise, and 
##
AI skills now add the ability to create out believable websites or credential stuff.


Traditional MFA like SMS and TOTP are vulnerable to new advances.

[Open source acclimation of AI](xtekky/gpt4free)

### Risks
- Persistant: Protect against new weapons from AI
- Circumstantial: Protect against AI endpoints to prevent abuse and misuse
- Nascent + developing: POtential future vector apps to protect

### Securing your app
Upgrade bot and headless browsing detection
Invest in phishing-resistant authentication like Passkeys
Use a game theoretical approach to defend your app
    - You don't need to be the most secure app in the world, but try to be more secure than others.

## Notes 2

### Key Themes
- Risks associated with AI in cybersecurity
- Common hacking and social engineering techniques
- Mitigation strategies for AI-related security risks

### Detailed Notes

#### Risks Associated with AI in Cybersecurity
- AI advancements are increasing the frequency and sophistication of phishing scams and social engineering attacks.
- High-profile incidents such as a car dealership scam in California and a deepfake video call resulting in a $25 million loss illustrate the growing threat.
- AI can trick even savvy users into falling for scams.

#### Common Hacking and Social Engineering Techniques
- Credential stuffing: Using stolen login credentials to access multiple sites.
- Phishing: Creating fake websites to trick users into providing login information.
  - Example: Using domains like "BankofAmericaSecureLogin.com" to deceive users.
- AI tools can be used to automate and scale phishing attacks.
- Chatbots like ChatGPT can be manipulated to generate plausible phishing scripts.

#### Mitigation Strategies for AI-Related Security Risks
- **User Education:** Educate users to recognize and avoid phishing scams and social engineering attacks.
- **Phishing-Resistant Authentication:**
  - Implement passkeys instead of traditional Multi-Factor Authentication (MFA).
  - MFA can be bypassed by phishing sites capturing session cookies.
- **Game Theoretical Approach:**
  - Ensure your security measures are robust enough to deter attackers by making it more difficult to breach than alternative targets.
- **Protect AI Inputs:**
  - Secure AI interfaces from being abused.
  - Open source projects like GPT-4 Free compile lists of sites with unrestricted access to AI, posing a risk.
- **Plan for Future Risks:**
  - Stay informed about emerging threats and adapt security measures accordingly.

#### Notable Statistics
- In 2022, 1 in 5 logins were takeover attempts.
- Almost 400 million people had their credentials leaked.
- 35% of new applications integrate AI in some form.

#### Practical Examples
- Microsoft case study: An MFA bypass was achieved through a phishing site interposing between the user and the legitimate website.
- Cloudflare bypass: Scripts can bypass security measures without deep engineering knowledge.

#### Recommendations
1. **User Education:** Regularly educate users about the latest phishing techniques and how to avoid them.
2. **Phishing-Resistant Authentication:** Use passkeys and other advanced authentication methods to protect against phishing attacks.
3. **Game Theoretical Security:** Make your applications more secure than others to deter attackers.

#### Conclusion
- AI's role in cybersecurity is evolving, making it crucial to stay updated on new threats and implement advanced security measures.
- While AI presents new risks, with proper strategies and user education, these risks can be mitigated effectively.
